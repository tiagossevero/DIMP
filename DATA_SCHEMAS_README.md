# Gerador de Data Schemas - Projeto DIMP

Scripts para gerar automaticamente os schemas de todas as tabelas do projeto DIMP.

## ğŸ“‹ O que Ã© gerado

Para cada uma das **13 tabelas** do projeto, o script gera:

1. **DESCRIBE FORMATTED** - Estrutura completa da tabela (colunas, tipos, metadados)
2. **SELECT * LIMIT 10** - Amostra de 10 registros para visualizar os dados

## ğŸ“‚ Tabelas Processadas

### Tabelas Originais (4)
- `teste.dimp_cnpj_base` - Base cadastral de empresas
- `teste.dimp_socios` - InformaÃ§Ãµes de sÃ³cios
- `teste.dimp_pagamentos_cnpj` - Pagamentos via CNPJ (com NF)
- `teste.dimp_pagamentos_cpf` - Pagamentos via CPF (sem NF)

### Tabelas IntermediÃ¡rias (9)
- `teste.dimp_score_final` â­ - Scores de risco (tabela principal)
- `teste.dimp_operacoes_suspeitas` - OperaÃ§Ãµes suspeitas
- `teste.dimp_socios_multiplas_empresas` - SÃ³cios em mÃºltiplas empresas
- `teste.dimp_comparacao_cnpj_cpf` - ComparaÃ§Ã£o CNPJ vs CPF
- `teste.dimp_func_score_final` - Scores de funcionÃ¡rios
- `teste.dimp_funcionarios_agregado` - AgregaÃ§Ã£o de funcionÃ¡rios
- `teste.dimp_func_rede_multiplas` - FuncionÃ¡rios em mÃºltiplas empresas
- `teste.dimp_func_top_suspeitos` - Top funcionÃ¡rios suspeitos
- `usr_sat_ods.vw_ods_contrib` - View externa de cadastro ODS

## ğŸš€ Como Usar

### OpÃ§Ã£o 1: Jupyter Notebook (Recomendado)

1. Abra o notebook:
   ```bash
   jupyter notebook generate_data_schemas.ipynb
   ```

2. Execute todas as cÃ©lulas em sequÃªncia (Cell > Run All)

3. Acompanhe o progresso em tempo real

### OpÃ§Ã£o 2: Script Python

1. Execute o script:
   ```bash
   python generate_data_schemas.py
   ```

2. Aguarde a conclusÃ£o (pode levar alguns minutos)

## ğŸ“ Estrutura de SaÃ­da

Os schemas serÃ£o salvos na pasta `data-schemas/`:

```
data-schemas/
â”œâ”€â”€ RESUMO.txt                                    # Resumo geral da execuÃ§Ã£o
â”œâ”€â”€ originais/                                    # 4 tabelas originais
â”‚   â”œâ”€â”€ teste_dimp_cnpj_base.txt
â”‚   â”œâ”€â”€ teste_dimp_socios.txt
â”‚   â”œâ”€â”€ teste_dimp_pagamentos_cnpj.txt
â”‚   â””â”€â”€ teste_dimp_pagamentos_cpf.txt
â””â”€â”€ intermediarias/                               # 9 tabelas intermediÃ¡rias
    â”œâ”€â”€ teste_dimp_score_final.txt
    â”œâ”€â”€ teste_dimp_operacoes_suspeitas.txt
    â”œâ”€â”€ teste_dimp_socios_multiplas_empresas.txt
    â”œâ”€â”€ teste_dimp_comparacao_cnpj_cpf.txt
    â”œâ”€â”€ teste_dimp_func_score_final.txt
    â”œâ”€â”€ teste_dimp_funcionarios_agregado.txt
    â”œâ”€â”€ teste_dimp_func_rede_multiplas.txt
    â”œâ”€â”€ teste_dimp_func_top_suspeitos.txt
    â””â”€â”€ usr_sat_ods_vw_ods_contrib.txt
```

## ğŸ“„ Formato dos Arquivos

Cada arquivo `.txt` contÃ©m:

```
-- DESCRIBE FORMATTED teste.dimp_score_final
-- Gerado em: 2025-11-17 14:30:00

col_name                            data_type
--------------------------------------------------------------------------------
cnpj                                string
nm_razao_social                     string
score_risco_final                   double
...


-- SELECT * FROM teste.dimp_score_final LIMIT 10
-- Gerado em: 2025-11-17 14:30:05

cnpj | nm_razao_social | score_risco_final | ...
--------------------------------------------------------
12345678000190 | EMPRESA EXEMPLO LTDA | 85.5 | ...
...
```

## âš™ï¸ Requisitos

- Acesso ao banco de dados Spark
- Biblioteca `utils.spark_utils_session` configurada
- PermissÃµes de leitura nas tabelas do schema `teste`
- PermissÃµes de leitura na view `usr_sat_ods.vw_ods_contrib`

## ğŸ”§ CustomizaÃ§Ã£o

### Alterar limite de registros

No cÃ³digo, altere a linha:
```python
df = spark.sql(f"SELECT * FROM {tabela} LIMIT 10")
```

Para:
```python
df = spark.sql(f"SELECT * FROM {tabela} LIMIT 50")  # 50 registros
```

### Adicionar/Remover tabelas

Edite o dicionÃ¡rio `TABELAS` no inÃ­cio do script:

```python
TABELAS = {
    'originais': [
        'teste.dimp_cnpj_base',
        'teste.sua_nova_tabela',  # Adicione aqui
    ],
    'intermediarias': [
        'teste.dimp_score_final',
        # ...
    ]
}
```

## ğŸ“Š InformaÃ§Ãµes Adicionais

### Tempo de ExecuÃ§Ã£o
- **Estimado**: 2-5 minutos (depende do tamanho das tabelas)
- Cada tabela leva ~10-30 segundos

### Tratamento de Erros
- Se uma tabela falhar, o script continua processando as demais
- Erros sÃ£o registrados nos arquivos `.txt` correspondentes
- Um resumo final indica quantas tabelas foram processadas com sucesso

### Logs
O script exibe logs detalhados:
```
======================================================================
Processando: teste.dimp_score_final
======================================================================
  â†’ DESCRIBE FORMATTED teste.dimp_score_final
  â†’ SELECT * FROM teste.dimp_score_final LIMIT 10
âœ“ Salvo em: data-schemas/intermediarias/teste_dimp_score_final.txt
```

## ğŸ†˜ SoluÃ§Ã£o de Problemas

### Erro: "Table not found"
- Verifique se vocÃª tem acesso ao schema `teste`
- Confirme se a tabela existe no banco

### Erro ao inicializar Spark
- Verifique suas credenciais de acesso
- Confirme se o mÃ³dulo `utils.spark_utils_session` estÃ¡ disponÃ­vel

### Pasta nÃ£o criada
- Verifique permissÃµes de escrita no diretÃ³rio atual
- Execute com `sudo` se necessÃ¡rio (nÃ£o recomendado)

## ğŸ“ PrÃ³ximos Passos

ApÃ³s gerar os schemas:

1. Revise os arquivos gerados em `data-schemas/`
2. Verifique se todas as 13 tabelas foram processadas
3. Use esses schemas para:
   - DocumentaÃ§Ã£o do projeto
   - AnÃ¡lise de estrutura de dados
   - Onboarding de novos desenvolvedores
   - Versionamento de schemas

## ğŸ“ Suporte

Em caso de dÃºvidas ou problemas:
1. Verifique o arquivo `data-schemas/RESUMO.txt`
2. Revise os logs de execuÃ§Ã£o
3. Consulte a documentaÃ§Ã£o do PySpark

---

**Gerado por**: Claude Code
**Data**: 2025-11-17
**VersÃ£o**: 1.0
