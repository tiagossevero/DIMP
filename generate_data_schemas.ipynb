{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerador de Data Schemas - Projeto DIMP\n",
    "\n",
    "Este notebook gera automaticamente os schemas de todas as tabelas do projeto DIMP.\n",
    "\n",
    "Para cada tabela, executa:\n",
    "- `DESCRIBE FORMATTED tabela`\n",
    "- `SELECT * FROM tabela LIMIT 10`\n",
    "\n",
    "## Tabelas a processar:\n",
    "\n",
    "### Originais (4):\n",
    "1. teste.dimp_cnpj_base\n",
    "2. teste.dimp_socios\n",
    "3. teste.dimp_pagamentos_cnpj\n",
    "4. teste.dimp_pagamentos_cpf\n",
    "\n",
    "### Intermediárias (9):\n",
    "5. teste.dimp_score_final\n",
    "6. teste.dimp_operacoes_suspeitas\n",
    "7. teste.dimp_socios_multiplas_empresas\n",
    "8. teste.dimp_comparacao_cnpj_cpf\n",
    "9. teste.dimp_func_score_final\n",
    "10. teste.dimp_funcionarios_agregado\n",
    "11. teste.dimp_func_rede_multiplas\n",
    "12. teste.dimp_func_top_suspeitos\n",
    "13. usr_sat_ods.vw_ods_contrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Importar utilitários Spark\n",
    "from utils import spark_utils_session as utils\n",
    "\n",
    "print(\"✓ Imports realizados com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definir Lista de Tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista completa de tabelas do projeto DIMP\n",
    "TABELAS = {\n",
    "    # Tabelas Originais (Fonte de Dados)\n",
    "    'originais': [\n",
    "        'teste.dimp_cnpj_base',\n",
    "        'teste.dimp_socios',\n",
    "        'teste.dimp_pagamentos_cnpj',\n",
    "        'teste.dimp_pagamentos_cpf',\n",
    "    ],\n",
    "    \n",
    "    # Tabelas Intermediárias/Processadas\n",
    "    'intermediarias': [\n",
    "        'teste.dimp_score_final',\n",
    "        'teste.dimp_operacoes_suspeitas',\n",
    "        'teste.dimp_socios_multiplas_empresas',\n",
    "        'teste.dimp_comparacao_cnpj_cpf',\n",
    "        'teste.dimp_func_score_final',\n",
    "        'teste.dimp_funcionarios_agregado',\n",
    "        'teste.dimp_func_rede_multiplas',\n",
    "        'teste.dimp_func_top_suspeitos',\n",
    "        'usr_sat_ods.vw_ods_contrib',\n",
    "    ]\n",
    "}\n",
    "\n",
    "total_tabelas = sum(len(tabelas) for tabelas in TABELAS.values())\n",
    "print(f\"Total de tabelas a processar: {total_tabelas}\")\n",
    "print(f\"  - Originais: {len(TABELAS['originais'])}\")\n",
    "print(f\"  - Intermediárias: {len(TABELAS['intermediarias'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inicializar Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session(profile: str = 'default', dynamic_allocation_enabled: bool = True):\n",
    "    \"\"\"Cria sessão Spark.\"\"\"\n",
    "    spark_builder = (utils.DBASparkAppSession\n",
    "                     .builder\n",
    "                     .appName('DIMP_Data_Schema_Generator')\n",
    "                     .language(utils.AvailableLanguages.PYTHON)\n",
    "                     .profileName(profile))\n",
    "    \n",
    "    if dynamic_allocation_enabled:\n",
    "        spark_builder.autoResourceManagement()\n",
    "    \n",
    "    return spark_builder.build()\n",
    "\n",
    "# Inicializar sessão\n",
    "print(\"Inicializando sessão Spark...\")\n",
    "session = get_session()\n",
    "spark = session.sparkSession\n",
    "print(\"✓ Sessão Spark iniciada com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Criar Estrutura de Pastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar estrutura de diretórios\n",
    "base_path = Path('data-schemas')\n",
    "\n",
    "dirs = [\n",
    "    base_path / 'originais',\n",
    "    base_path / 'intermediarias',\n",
    "]\n",
    "\n",
    "for dir_path in dirs:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"✓ {dir_path}\")\n",
    "\n",
    "print(f\"\\n✓ Estrutura criada em: {base_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executar_describe_formatted(spark, tabela: str) -> str:\n",
    "    \"\"\"Executa DESCRIBE FORMATTED e retorna resultado como string.\"\"\"\n",
    "    try:\n",
    "        print(f\"  → DESCRIBE FORMATTED {tabela}\")\n",
    "        df = spark.sql(f\"DESCRIBE FORMATTED {tabela}\")\n",
    "        \n",
    "        resultado = f\"-- DESCRIBE FORMATTED {tabela}\\n\"\n",
    "        resultado += f\"-- Gerado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "        \n",
    "        rows = df.collect()\n",
    "        for row in rows:\n",
    "            resultado += f\"{row[0]:<35} {row[1]:<40} {row[2] if len(row) > 2 else ''}\\n\"\n",
    "        \n",
    "        return resultado\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Erro: {str(e)}\")\n",
    "        return f\"-- ERRO: {str(e)}\\n\"\n",
    "\n",
    "\n",
    "def executar_select_sample(spark, tabela: str) -> str:\n",
    "    \"\"\"Executa SELECT * LIMIT 10 e retorna resultado como string.\"\"\"\n",
    "    try:\n",
    "        print(f\"  → SELECT * FROM {tabela} LIMIT 10\")\n",
    "        df = spark.sql(f\"SELECT * FROM {tabela} LIMIT 10\")\n",
    "        \n",
    "        resultado = f\"\\n\\n-- SELECT * FROM {tabela} LIMIT 10\\n\"\n",
    "        resultado += f\"-- Gerado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "        \n",
    "        colunas = df.columns\n",
    "        rows = df.collect()\n",
    "        \n",
    "        if not rows:\n",
    "            resultado += \"-- Tabela vazia (sem dados)\\n\"\n",
    "            return resultado\n",
    "        \n",
    "        resultado += \" | \".join(colunas) + \"\\n\"\n",
    "        resultado += \"-\" * (len(\" | \".join(colunas))) + \"\\n\"\n",
    "        \n",
    "        for row in rows:\n",
    "            valores = [str(row[col]) if row[col] is not None else 'NULL' for col in colunas]\n",
    "            resultado += \" | \".join(valores) + \"\\n\"\n",
    "        \n",
    "        return resultado\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Erro: {str(e)}\")\n",
    "        return f\"\\n\\n-- ERRO: {str(e)}\\n\"\n",
    "\n",
    "\n",
    "def gerar_schema_tabela(spark, tabela: str, tipo: str, base_path: Path):\n",
    "    \"\"\"Gera schema completo para uma tabela.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processando: {tabela}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Executar comandos\n",
    "    describe_result = executar_describe_formatted(spark, tabela)\n",
    "    select_result = executar_select_sample(spark, tabela)\n",
    "    \n",
    "    # Combinar resultados\n",
    "    conteudo_completo = describe_result + select_result\n",
    "    \n",
    "    # Salvar arquivo\n",
    "    nome_limpo = tabela.replace('.', '_')\n",
    "    arquivo = base_path / tipo / f\"{nome_limpo}.txt\"\n",
    "    arquivo.write_text(conteudo_completo, encoding='utf-8')\n",
    "    \n",
    "    print(f\"✓ Salvo em: {arquivo}\")\n",
    "    return arquivo\n",
    "\n",
    "print(\"✓ Funções definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PROCESSAR TABELAS ORIGINAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#\" * 70)\n",
    "print(\"# PROCESSANDO: TABELAS ORIGINAIS\")\n",
    "print(\"#\" * 70)\n",
    "\n",
    "for tabela in TABELAS['originais']:\n",
    "    try:\n",
    "        gerar_schema_tabela(spark, tabela, 'originais', base_path)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ ERRO em {tabela}: {e}\")\n",
    "\n",
    "print(\"\\n✓ Tabelas originais processadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PROCESSAR TABELAS INTERMEDIÁRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#\" * 70)\n",
    "print(\"# PROCESSANDO: TABELAS INTERMEDIÁRIAS\")\n",
    "print(\"#\" * 70)\n",
    "\n",
    "for tabela in TABELAS['intermediarias']:\n",
    "    try:\n",
    "        gerar_schema_tabela(spark, tabela, 'intermediarias', base_path)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ ERRO em {tabela}: {e}\")\n",
    "\n",
    "print(\"\\n✓ Tabelas intermediárias processadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Gerar Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar arquivos gerados\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ARQUIVOS GERADOS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nOriginais:\")\n",
    "for arquivo in sorted((base_path / 'originais').glob('*.txt')):\n",
    "    print(f\"  ✓ {arquivo.name}\")\n",
    "\n",
    "print(\"\\nIntermediárias:\")\n",
    "for arquivo in sorted((base_path / 'intermediarias').glob('*.txt')):\n",
    "    print(f\"  ✓ {arquivo.name}\")\n",
    "\n",
    "# Gerar arquivo de resumo\n",
    "resumo_path = base_path / 'RESUMO.txt'\n",
    "resumo = f\"\"\"\n",
    "RESUMO DA GERAÇÃO DE DATA SCHEMAS - PROJETO DIMP\n",
    "{'='*70}\n",
    "\n",
    "Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Total de tabelas: {total_tabelas}\n",
    "\n",
    "TABELAS ORIGINAIS ({len(TABELAS['originais'])}):\n",
    "{chr(10).join(f'  - {t}' for t in TABELAS['originais'])}\n",
    "\n",
    "TABELAS INTERMEDIÁRIAS ({len(TABELAS['intermediarias'])}):\n",
    "{chr(10).join(f'  - {t}' for t in TABELAS['intermediarias'])}\n",
    "\n",
    "{'='*70}\n",
    "Arquivos salvos em: {base_path.absolute()}\n",
    "\"\"\"\n",
    "\n",
    "resumo_path.write_text(resumo, encoding='utf-8')\n",
    "print(resumo)\n",
    "\n",
    "print(\"\\n✓ CONCLUÍDO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Finalizar Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizar sessão\n",
    "session.stop()\n",
    "print(\"✓ Sessão Spark finalizada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Processo Completo!\n",
    "\n",
    "Os schemas foram gerados na pasta `data-schemas/` com a seguinte estrutura:\n",
    "\n",
    "```\n",
    "data-schemas/\n",
    "├── RESUMO.txt\n",
    "├── originais/\n",
    "│   ├── teste_dimp_cnpj_base.txt\n",
    "│   ├── teste_dimp_socios.txt\n",
    "│   ├── teste_dimp_pagamentos_cnpj.txt\n",
    "│   └── teste_dimp_pagamentos_cpf.txt\n",
    "└── intermediarias/\n",
    "    ├── teste_dimp_score_final.txt\n",
    "    ├── teste_dimp_operacoes_suspeitas.txt\n",
    "    ├── teste_dimp_socios_multiplas_empresas.txt\n",
    "    ├── teste_dimp_comparacao_cnpj_cpf.txt\n",
    "    ├── teste_dimp_func_score_final.txt\n",
    "    ├── teste_dimp_funcionarios_agregado.txt\n",
    "    ├── teste_dimp_func_rede_multiplas.txt\n",
    "    ├── teste_dimp_func_top_suspeitos.txt\n",
    "    └── usr_sat_ods_vw_ods_contrib.txt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
