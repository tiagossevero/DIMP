{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aea681-2477-45f5-9ffe-1cb8f6cb1a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/plugins\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags\")\n",
    "\n",
    "#Import libs python\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "\n",
    "#Import libs internas\n",
    "from utils import spark_utils_session as utils\n",
    "\n",
    "from hooks.hdfs.hdfs_helper import HdfsHelper\n",
    "from jobs.job_base_config import BaseETLJobClass\n",
    "\n",
    "import poc_helper\n",
    "poc_helper.load_env(\"PROD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf8a1f-48e4-4c57-a162-c839c8e3a940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_session(profile: str, dynamic_allocation_enabled: bool = True) -> utils.DBASparkAppSession:\n",
    "    \"\"\"Generates DBASparkAppSession.\"\"\"\n",
    "    \n",
    "    app_name = \"tsevero_dimp\"\n",
    "    \n",
    "    spark_builder = (utils.DBASparkAppSession\n",
    "                     .builder\n",
    "                     .setAppName(app_name)\n",
    "                     .usingProcessProfile(profile)\n",
    "                    )\n",
    "    \n",
    "    if dynamic_allocation_enabled:\n",
    "        spark_builder.autoResourceManagement()\n",
    "\n",
    "    return spark_builder.build()\n",
    "\n",
    "session = get_session(profile='efd_t2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00777b24-69dc-4a6e-ba6f-4e11cd534937",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sparkSession.sql(\"SHOW DATABASES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5dd6bb-3844-436b-9810-cce86e02a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURA√á√ÉO INICIAL - PROJETO DIMP\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PySpark imports com aliases para evitar conflitos\n",
    "from pyspark.sql.functions import (\n",
    "    col as spark_col, \n",
    "    sum as spark_sum, \n",
    "    avg as spark_avg,\n",
    "    count as spark_count,\n",
    "    when as spark_when,\n",
    "    desc as spark_desc,\n",
    "    asc as spark_asc,\n",
    "    round as spark_round,\n",
    "    concat as spark_concat,\n",
    "    lit as spark_lit,\n",
    "    max as spark_max,\n",
    "    min as spark_min,\n",
    "    stddev as spark_stddev,\n",
    "    countDistinct as spark_countDistinct\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# ‚úÖ CORRE√á√ÉO: N√£o usar abs() que conflita com PySpark\n",
    "# pd.set_option('display.float_format', lambda x: f'{x:,.2f}' if abs(x) > 0.01 else f'{x:.6f}')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "# Acesso ao Spark\n",
    "spark = session.sparkSession\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç SISTEMA DE AN√ÅLISE DIMP - Meios de Pagamento por CPF/CNPJ\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Sess√£o Spark: {spark.sparkContext.appName}\")\n",
    "print(f\"Vers√£o Spark: {spark.version}\")\n",
    "print(f\"Iniciado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a087a-7358-496a-bd61-6fae7662584e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PANORAMA GERAL DO SISTEMA DIMP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä VERIFICA√á√ÉO DE TABELAS E ESTAT√çSTICAS GERAIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Lista de tabelas do projeto\n",
    "tabelas_dimp = [\n",
    "    'teste.dimp_cnpj_base',\n",
    "    'teste.dimp_socios',\n",
    "    'teste.dimp_pagamentos_cnpj',\n",
    "    'teste.dimp_pagamentos_cpf',\n",
    "    'teste.dimp_comparacao_cnpj_cpf',\n",
    "    'teste.dimp_score_final',\n",
    "    'teste.dimp_operacoes_suspeitas',\n",
    "    'teste.dimp_socios_multiplas_empresas'\n",
    "]\n",
    "\n",
    "print(\"\\nüìã Verificando exist√™ncia e tamanho das tabelas:\\n\")\n",
    "for tabela in tabelas_dimp:\n",
    "    try:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {tabela}\").collect()[0]['cnt']\n",
    "        print(f\"‚úÖ {tabela:50s} ‚Üí {count:>12,} registros\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {tabela:50s} ‚Üí N√ÉO ENCONTRADA\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìà ESTAT√çSTICAS GERAIS DO SISTEMA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Criar view tempor√°ria com estat√≠sticas gerais\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_panorama_geral AS\n",
    "SELECT \n",
    "    COUNT(DISTINCT cnpj) AS total_empresas,\n",
    "    COUNT(DISTINCT CASE WHEN classificacao_risco IN ('ALTO', 'M√âDIO-ALTO') THEN cnpj END) AS empresas_suspeitas,\n",
    "    CAST(COALESCE(SUM(total_geral), 0) AS DOUBLE) AS volume_total,\n",
    "    CAST(COALESCE(SUM(total_recebido_cpf), 0) AS DOUBLE) AS volume_cpf,\n",
    "    CAST(COALESCE(SUM(total_recebido_cnpj), 0) AS DOUBLE) AS volume_cnpj,\n",
    "    CAST(COALESCE(AVG(perc_recebido_cpf), 0) AS DOUBLE) AS media_perc_cpf,\n",
    "    CAST(COALESCE(AVG(score_risco_final), 0) AS DOUBLE) AS media_score,\n",
    "    COUNT(DISTINCT CASE WHEN perc_recebido_cpf >= 80 THEN cnpj END) AS empresas_80pct_cpf,\n",
    "    COUNT(DISTINCT CASE WHEN perc_recebido_cpf >= 50 THEN cnpj END) AS empresas_50pct_cpf\n",
    "FROM teste.dimp_score_final\n",
    "\"\"\")\n",
    "\n",
    "# Verificar tamanho\n",
    "total_panorama = spark.sql(\"SELECT COUNT(*) as cnt FROM vw_panorama_geral\").collect()[0]['cnt']\n",
    "print(f\"\\nüìä Total de registros no panorama: {total_panorama}\")\n",
    "\n",
    "# Converter para pandas (√© pequeno, s√≥ 1 linha)\n",
    "df_panorama = spark.sql(\"SELECT * FROM vw_panorama_geral\").toPandas()\n",
    "\n",
    "if len(df_panorama) > 0:\n",
    "    p = df_panorama.iloc[0]\n",
    "    \n",
    "    print(\"\\nüéØ M√âTRICAS PRINCIPAIS:\")\n",
    "    print(f\"  ‚Ä¢ Total de Empresas Analisadas: {int(p['total_empresas']):,}\")\n",
    "    print(f\"  ‚Ä¢ Empresas Suspeitas (Alto/M√©dio-Alto): {int(p['empresas_suspeitas']):,}\")\n",
    "    print(f\"  ‚Ä¢ Volume Total Movimentado: R$ {p['volume_total']:,.2f}\")\n",
    "    print(f\"  ‚Ä¢ Volume em CPF (S√≥cios): R$ {p['volume_cpf']:,.2f}\")\n",
    "    print(f\"  ‚Ä¢ Volume em CNPJ (Empresa): R$ {p['volume_cnpj']:,.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìä INDICADORES DE RISCO:\")\n",
    "    print(f\"  ‚Ä¢ % M√©dio Recebido em CPF: {p['media_perc_cpf']:.2f}%\")\n",
    "    print(f\"  ‚Ä¢ Score M√©dio de Risco: {p['media_score']:.2f}\")\n",
    "    print(f\"  ‚Ä¢ Empresas com ‚â•80% CPF: {int(p['empresas_80pct_cpf']):,}\")\n",
    "    print(f\"  ‚Ä¢ Empresas com ‚â•50% CPF: {int(p['empresas_50pct_cpf']):,}\")\n",
    "    \n",
    "    # Calcular % de risco\n",
    "    perc_suspeitas = (int(p['empresas_suspeitas']) / int(p['total_empresas'])) * 100 if int(p['total_empresas']) > 0 else 0\n",
    "    perc_cpf = (p['volume_cpf'] / p['volume_total']) * 100 if p['volume_total'] > 0 else 0\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  ALERTAS:\")\n",
    "    print(f\"  ‚Ä¢ {perc_suspeitas:.1f}% das empresas s√£o suspeitas\")\n",
    "    print(f\"  ‚Ä¢ {perc_cpf:.1f}% do volume total passa por CPF de s√≥cios\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado encontrado no panorama geral\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac160b8-35ea-4fd9-acd8-7f1d2c9aee53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DISTRIBUI√á√ÉO DE RISCO - AN√ÅLISE POR CLASSIFICA√á√ÉO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä DISTRIBUI√á√ÉO DE EMPRESAS POR CLASSIFICA√á√ÉO DE RISCO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Criar view de distribui√ß√£o\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_distribuicao_risco AS\n",
    "SELECT \n",
    "    classificacao_risco,\n",
    "    COUNT(DISTINCT cnpj) AS qtd_empresas,\n",
    "    CAST(COALESCE(SUM(total_geral), 0) AS DOUBLE) AS volume_total,\n",
    "    CAST(COALESCE(SUM(total_recebido_cpf), 0) AS DOUBLE) AS volume_cpf,\n",
    "    CAST(COALESCE(AVG(perc_recebido_cpf), 0) AS DOUBLE) AS media_perc_cpf,\n",
    "    CAST(COALESCE(AVG(score_risco_final), 0) AS DOUBLE) AS media_score\n",
    "FROM teste.dimp_score_final\n",
    "GROUP BY classificacao_risco\n",
    "ORDER BY \n",
    "    CASE classificacao_risco\n",
    "        WHEN 'ALTO' THEN 1\n",
    "        WHEN 'M√âDIO-ALTO' THEN 2\n",
    "        WHEN 'M√âDIO' THEN 3\n",
    "        ELSE 4\n",
    "    END\n",
    "\"\"\")\n",
    "\n",
    "# Verificar tamanho e converter\n",
    "total_dist = spark.sql(\"SELECT COUNT(*) as cnt FROM vw_distribuicao_risco\").collect()[0]['cnt']\n",
    "print(f\"üìä Total de classifica√ß√µes: {total_dist}\")\n",
    "\n",
    "df_dist = spark.sql(\"SELECT * FROM vw_distribuicao_risco\").toPandas()\n",
    "\n",
    "print(\"\\nüìã Distribui√ß√£o por Classifica√ß√£o:\\n\")\n",
    "for idx, row in df_dist.iterrows():\n",
    "    print(f\"  {row['classificacao_risco']:12s} ‚Üí {int(row['qtd_empresas']):>6,} empresas | \"\n",
    "          f\"Volume: R$ {row['volume_total']:>15,.2f} | \"\n",
    "          f\"Score M√©dio: {row['media_score']:>6.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# GR√ÅFICOS INTERATIVOS COM PLOTLY\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Pizza - Distribui√ß√£o de Empresas\n",
    "fig_pizza = go.Figure(data=[go.Pie(\n",
    "    labels=df_dist['classificacao_risco'],\n",
    "    values=df_dist['qtd_empresas'],\n",
    "    hole=0.4,\n",
    "    marker=dict(colors=['#d62728', '#ff7f0e', '#ffdd70', '#2ca02c']),\n",
    "    textinfo='label+percent+value',\n",
    "    texttemplate='<b>%{label}</b><br>%{value:,} empresas<br>%{percent:.1%}'\n",
    ")])\n",
    "\n",
    "fig_pizza.update_layout(\n",
    "    title='<b>Distribui√ß√£o de Empresas por N√≠vel de Risco</b>',\n",
    "    height=500,\n",
    "    showlegend=True,\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "fig_pizza.show()\n",
    "\n",
    "# 2. Barras Horizontais - Volume por Classifica√ß√£o\n",
    "fig_barras = go.Figure()\n",
    "\n",
    "fig_barras.add_trace(go.Bar(\n",
    "    y=df_dist['classificacao_risco'],\n",
    "    x=df_dist['volume_cpf'] / 1e6,\n",
    "    name='Volume CPF',\n",
    "    orientation='h',\n",
    "    marker=dict(color='#ff7f0e'),\n",
    "    text=df_dist['volume_cpf'].apply(lambda x: f'R$ {x/1e6:.1f}M'),\n",
    "    textposition='inside'\n",
    "))\n",
    "\n",
    "fig_barras.add_trace(go.Bar(\n",
    "    y=df_dist['classificacao_risco'],\n",
    "    x=df_dist['volume_total'] / 1e6,\n",
    "    name='Volume Total',\n",
    "    orientation='h',\n",
    "    marker=dict(color='#1f77b4'),\n",
    "    text=df_dist['volume_total'].apply(lambda x: f'R$ {x/1e6:.1f}M'),\n",
    "    textposition='inside'\n",
    "))\n",
    "\n",
    "fig_barras.update_layout(\n",
    "    title='<b>Volume Financeiro por Classifica√ß√£o de Risco</b>',\n",
    "    xaxis_title='Volume (Milh√µes R$)',\n",
    "    yaxis_title='Classifica√ß√£o',\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig_barras.show()\n",
    "\n",
    "# 3. Gauge - Score M√©dio Geral\n",
    "score_geral = df_dist['media_score'].mean()\n",
    "\n",
    "fig_gauge = go.Figure(go.Indicator(\n",
    "    mode=\"gauge+number+delta\",\n",
    "    value=score_geral,\n",
    "    domain={'x': [0, 1], 'y': [0, 1]},\n",
    "    title={'text': \"<b>Score M√©dio de Risco Geral</b>\", 'font': {'size': 24}},\n",
    "    delta={'reference': 50},\n",
    "    gauge={\n",
    "        'axis': {'range': [None, 100], 'tickwidth': 1, 'tickcolor': \"darkblue\"},\n",
    "        'bar': {'color': \"darkblue\"},\n",
    "        'bgcolor': \"white\",\n",
    "        'borderwidth': 2,\n",
    "        'bordercolor': \"gray\",\n",
    "        'steps': [\n",
    "            {'range': [0, 40], 'color': '#2ca02c'},\n",
    "            {'range': [40, 60], 'color': '#ffdd70'},\n",
    "            {'range': [60, 80], 'color': '#ff7f0e'},\n",
    "            {'range': [80, 100], 'color': '#d62728'}\n",
    "        ],\n",
    "        'threshold': {\n",
    "            'line': {'color': \"red\", 'width': 4},\n",
    "            'thickness': 0.75,\n",
    "            'value': 80\n",
    "        }\n",
    "    }\n",
    "))\n",
    "\n",
    "fig_gauge.update_layout(height=400)\n",
    "fig_gauge.show()\n",
    "\n",
    "print(\"\\n‚úÖ Gr√°ficos interativos gerados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4c96c-85d4-428d-866f-62e4d7c7b9cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RANKING: TOP 50 EMPRESAS COM MAIOR RISCO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ TOP 50 EMPRESAS COM MAIOR SCORE DE RISCO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Criar view com top 50\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_top50_suspeitas AS\n",
    "SELECT \n",
    "    cnpj,\n",
    "    nm_razao_social,\n",
    "    regime_tributario,\n",
    "    COALESCE(municipio, 'N√ÉO INFORMADO') AS municipio,\n",
    "    COALESCE(uf, 'N/A') AS uf,\n",
    "    COALESCE(nm_cnae1, 'N√ÉO INFORMADO') AS nm_cnae1,\n",
    "    CAST(COALESCE(total_recebido_cnpj, 0) AS DOUBLE) AS total_cnpj,\n",
    "    CAST(COALESCE(total_recebido_cpf, 0) AS DOUBLE) AS total_cpf,\n",
    "    CAST(COALESCE(total_geral, 0) AS DOUBLE) AS total_geral,\n",
    "    CAST(COALESCE(perc_recebido_cpf, 0) AS DOUBLE) AS perc_cpf,\n",
    "    qtd_socios_recebendo,\n",
    "    meses_com_pagto_cpf,\n",
    "    CAST(COALESCE(score_risco_final, 0) AS DOUBLE) AS score_final,\n",
    "    classificacao_risco,\n",
    "    CAST(COALESCE(score_proporcao, 0) AS DOUBLE) AS score_proporcao,\n",
    "    CAST(COALESCE(score_volume_cpf, 0) AS DOUBLE) AS score_volume,\n",
    "    CAST(COALESCE(score_qtd_socios, 0) AS DOUBLE) AS score_socios,\n",
    "    CAST(COALESCE(score_desvio_regime, 0) AS DOUBLE) AS score_desvio,\n",
    "    CAST(COALESCE(score_consistencia, 0) AS DOUBLE) AS score_consistencia\n",
    "FROM teste.dimp_score_final\n",
    "WHERE classificacao_risco IN ('ALTO', 'M√âDIO-ALTO')\n",
    "ORDER BY score_risco_final DESC\n",
    "LIMIT 50\n",
    "\"\"\")\n",
    "\n",
    "# Verificar e converter\n",
    "total_top50 = spark.sql(\"SELECT COUNT(*) as cnt FROM vw_top50_suspeitas\").collect()[0]['cnt']\n",
    "print(f\"\\nüìä Total de empresas no ranking: {total_top50}\")\n",
    "\n",
    "df_top50 = spark.sql(\"SELECT * FROM vw_top50_suspeitas\").toPandas()\n",
    "\n",
    "print(f\"\\nüîù TOP 20 EMPRESAS MAIS SUSPEITAS:\\n\")\n",
    "for idx, row in df_top50.head(20).iterrows():\n",
    "    print(f\"{idx+1:2d}. CNPJ: {row['cnpj']}\")\n",
    "    print(f\"    Raz√£o Social: {str(row['nm_razao_social'])[:60]}\")\n",
    "    print(f\"    Score: {row['score_final']:.2f} | Risco: {row['classificacao_risco']}\")\n",
    "    print(f\"    % CPF: {row['perc_cpf']:.1f}% | Total: R$ {row['total_geral']:,.2f}\")\n",
    "    \n",
    "    # ‚úÖ CORRE√á√ÉO: Verificar se nm_cnae1 n√£o √© None antes de fatiar\n",
    "    cnae_str = str(row['nm_cnae1'])[:40] if row['nm_cnae1'] is not None else 'N/A'\n",
    "    print(f\"    Munic√≠pio: {row['municipio']} - {row['uf']} | CNAE: {cnae_str}\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# HEATMAP: Composi√ß√£o do Score - Top 20\n",
    "# ============================================================================\n",
    "\n",
    "# Preparar matriz de scores\n",
    "df_scores = df_top50.head(20)[['cnpj', 'score_proporcao', 'score_volume', \n",
    "                                'score_socios', 'score_desvio', 'score_consistencia']].copy()\n",
    "\n",
    "df_scores['cnpj_label'] = df_scores['cnpj'].str[-4:]  # √öltimos 4 d√≠gitos\n",
    "df_scores_matrix = df_scores.set_index('cnpj_label')[['score_proporcao', 'score_volume', \n",
    "                                                        'score_socios', 'score_desvio', 'score_consistencia']]\n",
    "\n",
    "# Renomear colunas\n",
    "df_scores_matrix.columns = ['Propor√ß√£o\\nCPF', 'Volume\\nCPF', 'Qtd\\nS√≥cios', \n",
    "                            'Desvio\\nRegime', 'Consist√™ncia\\nTemporal']\n",
    "\n",
    "fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "    z=df_scores_matrix.values.T,\n",
    "    x=df_scores_matrix.index,\n",
    "    y=df_scores_matrix.columns,\n",
    "    colorscale='YlOrRd',\n",
    "    text=df_scores_matrix.values.T,\n",
    "    texttemplate='%{text:.0f}',\n",
    "    textfont={\"size\": 10},\n",
    "    colorbar=dict(title=\"Score\")\n",
    "))\n",
    "\n",
    "fig_heatmap.update_layout(\n",
    "    title='<b>Composi√ß√£o do Score de Risco - Top 20 Empresas</b><br><sub>√öltimos 4 d√≠gitos do CNPJ</sub>',\n",
    "    xaxis_title='CNPJ (final)',\n",
    "    yaxis_title='Componente do Score',\n",
    "    height=500,\n",
    "    font=dict(size=11)\n",
    ")\n",
    "\n",
    "fig_heatmap.show()\n",
    "\n",
    "# ============================================================================\n",
    "# GR√ÅFICO: Score Final - Top 30\n",
    "# ============================================================================\n",
    "\n",
    "df_top30_chart = df_top50.head(30).sort_values('score_final', ascending=True)\n",
    "df_top30_chart['cnpj_curto'] = df_top30_chart['cnpj'].str[-6:]\n",
    "\n",
    "colors_risk = ['#8b0000' if x >= 90 else '#d62728' if x >= 80 else '#ff7f0e' \n",
    "               for x in df_top30_chart['score_final']]\n",
    "\n",
    "fig_bar_score = go.Figure(go.Bar(\n",
    "    y=df_top30_chart['cnpj_curto'],\n",
    "    x=df_top30_chart['score_final'],\n",
    "    orientation='h',\n",
    "    marker=dict(color=colors_risk),\n",
    "    text=df_top30_chart['score_final'].apply(lambda x: f'{x:.1f}'),\n",
    "    textposition='outside',\n",
    "    hovertemplate='<b>CNPJ (final): %{y}</b><br>Score: %{x:.2f}<br><extra></extra>'\n",
    "))\n",
    "\n",
    "fig_bar_score.update_layout(\n",
    "    title='<b>Score de Risco Final - Top 30 Empresas</b>',\n",
    "    xaxis_title='Score de Risco',\n",
    "    yaxis_title='CNPJ (6 √∫ltimos d√≠gitos)',\n",
    "    height=800,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig_bar_score.show()\n",
    "\n",
    "print(\"\\n‚úÖ An√°lise do ranking conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a369b-066f-4959-8d08-0485e3127861",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AN√ÅLISE TEMPORAL: EVOLU√á√ÉO DOS PAGAMENTOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìà AN√ÅLISE TEMPORAL - EVOLU√á√ÉO DOS PAGAMENTOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ‚úÖ CORRE√á√ÉO: Criar views separadas para CNPJ e CPF\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_temporal_cnpj AS\n",
    "SELECT \n",
    "    referencia,\n",
    "    COUNT(DISTINCT cnpj) AS qtd_empresas,\n",
    "    CAST(COALESCE(SUM(vl_total), 0) AS DOUBLE) AS volume_total\n",
    "FROM teste.dimp_pagamentos_cnpj\n",
    "GROUP BY referencia\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_temporal_cpf AS\n",
    "SELECT \n",
    "    referencia,\n",
    "    COUNT(DISTINCT cnpj) AS qtd_empresas,\n",
    "    CAST(COALESCE(SUM(vl_total), 0) AS DOUBLE) AS volume_total\n",
    "FROM teste.dimp_pagamentos_cpf\n",
    "GROUP BY referencia\n",
    "\"\"\")\n",
    "\n",
    "# Juntar as duas views\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_evolucao_temporal AS\n",
    "SELECT \n",
    "    COALESCE(c.referencia, p.referencia) AS referencia,\n",
    "    COALESCE(c.qtd_empresas, 0) AS empresas_cnpj,\n",
    "    COALESCE(p.qtd_empresas, 0) AS empresas_cpf,\n",
    "    COALESCE(c.volume_total, 0) AS vol_cnpj,\n",
    "    COALESCE(p.volume_total, 0) AS vol_cpf\n",
    "FROM vw_temporal_cnpj c\n",
    "FULL OUTER JOIN vw_temporal_cpf p ON c.referencia = p.referencia\n",
    "ORDER BY referencia\n",
    "\"\"\")\n",
    "\n",
    "# Verificar tamanho\n",
    "total_temporal = spark.sql(\"SELECT COUNT(*) as cnt FROM vw_evolucao_temporal\").collect()[0]['cnt']\n",
    "print(f\"üìä Total de per√≠odos analisados: {total_temporal}\")\n",
    "\n",
    "if total_temporal > 0 and total_temporal <= 100:\n",
    "    df_temporal = spark.sql(\"SELECT * FROM vw_evolucao_temporal\").toPandas()\n",
    "    \n",
    "    # Converter refer√™ncia para datetime\n",
    "    df_temporal['data'] = pd.to_datetime(df_temporal['referencia'].astype(str), format='%Y%m')\n",
    "    df_temporal['mes_ano'] = df_temporal['data'].dt.strftime('%Y-%m')\n",
    "    \n",
    "    print(f\"\\nüìÖ Per√≠odo analisado: {df_temporal['mes_ano'].min()} at√© {df_temporal['mes_ano'].max()}\")\n",
    "    print(f\"üìä Total de meses: {len(df_temporal)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # GR√ÅFICO: Evolu√ß√£o do Volume\n",
    "    # ========================================================================\n",
    "    \n",
    "    fig_evolucao = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=('Volume Financeiro Mensal', 'Quantidade de Empresas'),\n",
    "        vertical_spacing=0.12,\n",
    "        row_heights=[0.6, 0.4]\n",
    "    )\n",
    "    \n",
    "    # Volume financeiro\n",
    "    fig_evolucao.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_temporal['mes_ano'],\n",
    "            y=df_temporal['vol_cnpj'] / 1e6,\n",
    "            name='Volume CNPJ',\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='#1f77b4', width=2),\n",
    "            fill='tozeroy'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig_evolucao.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_temporal['mes_ano'],\n",
    "            y=df_temporal['vol_cpf'] / 1e6,\n",
    "            name='Volume CPF',\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='#ff7f0e', width=2),\n",
    "            fill='tozeroy'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Quantidade de empresas\n",
    "    fig_evolucao.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_temporal['mes_ano'],\n",
    "            y=df_temporal['empresas_cnpj'],\n",
    "            name='Empresas (CNPJ)',\n",
    "            marker=dict(color='#1f77b4')\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig_evolucao.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_temporal['mes_ano'],\n",
    "            y=df_temporal['empresas_cpf'],\n",
    "            name='Empresas (CPF)',\n",
    "            marker=dict(color='#ff7f0e')\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig_evolucao.update_xaxes(title_text=\"M√™s/Ano\", row=2, col=1)\n",
    "    fig_evolucao.update_yaxes(title_text=\"Volume (Milh√µes R$)\", row=1, col=1)\n",
    "    fig_evolucao.update_yaxes(title_text=\"Quantidade\", row=2, col=1)\n",
    "    \n",
    "    fig_evolucao.update_layout(\n",
    "        title='<b>Evolu√ß√£o Temporal dos Pagamentos (2024-2025)</b>',\n",
    "        height=700,\n",
    "        showlegend=True,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    fig_evolucao.show()\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    print(f\"\\nüìä ESTAT√çSTICAS TEMPORAIS:\")\n",
    "    print(f\"  ‚Ä¢ Volume M√©dio Mensal (CNPJ): R$ {df_temporal['vol_cnpj'].mean():,.2f}\")\n",
    "    print(f\"  ‚Ä¢ Volume M√©dio Mensal (CPF): R$ {df_temporal['vol_cpf'].mean():,.2f}\")\n",
    "    \n",
    "    # Crescimento (se houver dados suficientes)\n",
    "    if len(df_temporal) >= 2:\n",
    "        if df_temporal['vol_cpf'].iloc[0] > 0:\n",
    "            crescimento_cpf = ((df_temporal['vol_cpf'].iloc[-1] / df_temporal['vol_cpf'].iloc[0]) - 1) * 100\n",
    "            print(f\"  ‚Ä¢ Crescimento CPF: {crescimento_cpf:.1f}%\")\n",
    "        \n",
    "        if df_temporal['vol_cnpj'].iloc[0] > 0:\n",
    "            crescimento_cnpj = ((df_temporal['vol_cnpj'].iloc[-1] / df_temporal['vol_cnpj'].iloc[0]) - 1) * 100\n",
    "            print(f\"  ‚Ä¢ Crescimento CNPJ: {crescimento_cnpj:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Dados temporais n√£o dispon√≠veis ou muito volumosos ({total_temporal} registros)\")\n",
    "\n",
    "print(\"\\n‚úÖ An√°lise temporal conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce46934-8e8d-42da-9b53-89d2f6e42b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AN√ÅLISE GEOGR√ÅFICA: DISTRIBUI√á√ÉO POR UF E MUNIC√çPIO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üó∫Ô∏è  AN√ÅLISE GEOGR√ÅFICA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Criar view geogr√°fica\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_geografica AS\n",
    "SELECT \n",
    "    uf,\n",
    "    municipio,\n",
    "    COUNT(DISTINCT cnpj) AS qtd_empresas,\n",
    "    CAST(COALESCE(SUM(total_geral), 0) AS DOUBLE) AS volume_total,\n",
    "    CAST(COALESCE(SUM(total_recebido_cpf), 0) AS DOUBLE) AS volume_cpf,\n",
    "    CAST(COALESCE(AVG(perc_recebido_cpf), 0) AS DOUBLE) AS media_perc_cpf,\n",
    "    CAST(COALESCE(AVG(score_risco_final), 0) AS DOUBLE) AS media_score,\n",
    "    COUNT(DISTINCT CASE WHEN classificacao_risco = 'ALTO' THEN cnpj END) AS empresas_alto_risco\n",
    "FROM teste.dimp_score_final\n",
    "WHERE uf IS NOT NULL AND uf != ''\n",
    "GROUP BY uf, municipio\n",
    "\"\"\")\n",
    "\n",
    "# Verificar tamanho\n",
    "total_geo = spark.sql(\"SELECT COUNT(*) as cnt FROM vw_geografica\").collect()[0]['cnt']\n",
    "print(f\"üìä Total de localidades: {total_geo}\")\n",
    "\n",
    "# Agrega√ß√£o por UF (sempre pequena)\n",
    "df_uf = (spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        uf,\n",
    "        SUM(qtd_empresas) AS total_empresas,\n",
    "        SUM(volume_total) AS volume_total,\n",
    "        SUM(volume_cpf) AS volume_cpf,\n",
    "        AVG(media_score) AS score_medio,\n",
    "        SUM(empresas_alto_risco) AS total_alto_risco\n",
    "    FROM vw_geografica\n",
    "    GROUP BY uf\n",
    "    ORDER BY volume_total DESC\n",
    "\"\"\")\n",
    ".limit(27)  # M√°ximo de UFs no Brasil\n",
    ".toPandas())\n",
    "\n",
    "print(f\"\\nüìç DISTRIBUI√á√ÉO POR ESTADO:\\n\")\n",
    "for idx, row in df_uf.head(10).iterrows():\n",
    "    perc_risco = (row['total_alto_risco'] / row['total_empresas']) * 100 if row['total_empresas'] > 0 else 0\n",
    "    print(f\"  {row['uf']:2s} ‚Üí {int(row['total_empresas']):>5,} empresas | \"\n",
    "          f\"Volume: R$ {row['volume_total']:>15,.2f} | \"\n",
    "          f\"Score: {row['score_medio']:>6.2f} | \"\n",
    "          f\"Alto Risco: {perc_risco:>5.1f}%\")\n",
    "\n",
    "# ========================================================================\n",
    "# GR√ÅFICO: Mapa de Calor por UF\n",
    "# ========================================================================\n",
    "\n",
    "fig_mapa_uf = go.Figure(data=go.Choropleth(\n",
    "    locations=df_uf['uf'],\n",
    "    z=df_uf['score_medio'],\n",
    "    locationmode='USA-states',  # Funciona para siglas de estados\n",
    "    colorscale='YlOrRd',\n",
    "    text=df_uf['uf'],\n",
    "    marker_line_color='white',\n",
    "    colorbar_title=\"Score<br>M√©dio\"\n",
    "))\n",
    "\n",
    "fig_mapa_uf.update_layout(\n",
    "    title='<b>Score M√©dio de Risco por Estado</b>',\n",
    "    geo=dict(\n",
    "        scope='south america',\n",
    "        showlakes=True,\n",
    "        lakecolor='rgb(255, 255, 255)'\n",
    "    ),\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig_mapa_uf.show()\n",
    "\n",
    "# ========================================================================\n",
    "# Top Munic√≠pios\n",
    "# ========================================================================\n",
    "\n",
    "if total_geo <= 1000:\n",
    "    df_municipios = (spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM vw_geografica\n",
    "        ORDER BY volume_total DESC\n",
    "        LIMIT 20\n",
    "    \"\"\").toPandas())\n",
    "    \n",
    "    print(f\"\\nüèôÔ∏è  TOP 20 MUNIC√çPIOS POR VOLUME:\\n\")\n",
    "    for idx, row in df_municipios.iterrows():\n",
    "        print(f\"  {idx+1:2d}. {row['municipio'][:30]:30s} ({row['uf']}) ‚Üí \"\n",
    "              f\"{int(row['qtd_empresas']):>4,} empresas | \"\n",
    "              f\"R$ {row['volume_total']:>15,.2f}\")\n",
    "    \n",
    "    # Gr√°fico\n",
    "    fig_municipios = go.Figure(go.Bar(\n",
    "        y=df_municipios['municipio'].str[:25] + ' (' + df_municipios['uf'] + ')',\n",
    "        x=df_municipios['volume_total'] / 1e6,\n",
    "        orientation='h',\n",
    "        marker=dict(color=df_municipios['media_score'], colorscale='YlOrRd',\n",
    "                   showscale=True, colorbar=dict(title=\"Score\")),\n",
    "        text=df_municipios['volume_total'].apply(lambda x: f'R$ {x/1e6:.1f}M'),\n",
    "        textposition='outside'\n",
    "    ))\n",
    "    \n",
    "    fig_municipios.update_layout(\n",
    "        title='<b>Top 20 Munic√≠pios por Volume Total</b>',\n",
    "        xaxis_title='Volume (Milh√µes R$)',\n",
    "        yaxis_title='Munic√≠pio',\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig_municipios.show()\n",
    "\n",
    "print(\"\\n‚úÖ An√°lise geogr√°fica conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b63148-623e-4f79-82cc-9d2751706d32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AN√ÅLISE DE REDE: S√ìCIOS EM M√öLTIPLAS EMPRESAS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üï∏Ô∏è  AN√ÅLISE DE REDE - S√≥cios com M√∫ltiplas Empresas\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Criar view de s√≥cios\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_socios_rede AS\n",
    "SELECT \n",
    "    cpf_socio,\n",
    "    nome_socio,\n",
    "    qtd_empresas,\n",
    "    CAST(COALESCE(total_recebido, 0) AS DOUBLE) AS total_recebido,\n",
    "    nivel_dispersao,\n",
    "    cnpjs_relacionados\n",
    "FROM teste.dimp_socios_multiplas_empresas\n",
    "ORDER BY qtd_empresas DESC, total_recebido DESC\n",
    "LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "# Verificar e converter\n",
    "total_socios = spark.sql(\"SELECT COUNT(*) as cnt FROM vw_socios_rede\").collect()[0]['cnt']\n",
    "print(f\"üìä Total de s√≥cios em m√∫ltiplas empresas: {total_socios}\")\n",
    "\n",
    "if total_socios > 0:\n",
    "    df_socios = spark.sql(\"SELECT * FROM vw_socios_rede\").toPandas()\n",
    "    \n",
    "    print(f\"\\nüë• TOP 20 S√ìCIOS COM MAIOR DISPERS√ÉO:\\n\")\n",
    "    for idx, row in df_socios.head(20).iterrows():\n",
    "        cpf_mask = row['cpf_socio'][:3] + '.***.***-' + row['cpf_socio'][-2:] if len(str(row['cpf_socio'])) == 11 else '***'\n",
    "        print(f\"{idx+1:2d}. CPF: {cpf_mask} | Nome: {str(row['nome_socio'])[:40]:40s}\")\n",
    "        print(f\"    Empresas: {int(row['qtd_empresas']):>3} | \"\n",
    "              f\"Volume: R$ {row['total_recebido']:>15,.2f} | \"\n",
    "              f\"Dispers√£o: {row['nivel_dispersao']}\")\n",
    "        print()\n",
    "    \n",
    "    # ====================================================================\n",
    "    # GR√ÅFICOS: An√°lise de Rede\n",
    "    # ====================================================================\n",
    "    \n",
    "    # 1. Distribui√ß√£o de Dispers√£o\n",
    "    dist_dispersao = df_socios['nivel_dispersao'].value_counts().sort_index()\n",
    "    \n",
    "    fig_dispersao = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=dist_dispersao.index,\n",
    "            y=dist_dispersao.values,\n",
    "            marker=dict(color=['#2ca02c', '#ffdd70', '#ff7f0e', '#d62728'][:len(dist_dispersao)]),\n",
    "            text=dist_dispersao.values,\n",
    "            textposition='outside'\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig_dispersao.update_layout(\n",
    "        title='<b>Distribui√ß√£o de S√≥cios por N√≠vel de Dispers√£o</b>',\n",
    "        xaxis_title='N√≠vel de Dispers√£o',\n",
    "        yaxis_title='Quantidade de S√≥cios',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig_dispersao.show()\n",
    "    \n",
    "    # 2. Scatter: Empresas vs Volume\n",
    "    fig_scatter = go.Figure(data=go.Scatter(\n",
    "        x=df_socios['qtd_empresas'],\n",
    "        y=df_socios['total_recebido'] / 1e3,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=df_socios['qtd_empresas'] * 2,\n",
    "            color=df_socios['qtd_empresas'],\n",
    "            colorscale='YlOrRd',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Qtd<br>Empresas\")\n",
    "        ),\n",
    "        text=df_socios['nome_socio'].str[:30],\n",
    "        hovertemplate='<b>%{text}</b><br>Empresas: %{x}<br>Volume: R$ %{y:.0f}k<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig_scatter.update_layout(\n",
    "        title='<b>Rela√ß√£o: Quantidade de Empresas vs Volume Recebido</b>',\n",
    "        xaxis_title='Quantidade de Empresas',\n",
    "        yaxis_title='Volume Total Recebido (Mil R$)',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig_scatter.show()\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    print(f\"\\nüìä ESTAT√çSTICAS DA REDE:\")\n",
    "    print(f\"  ‚Ä¢ Total de s√≥cios em m√∫ltiplas empresas: {len(df_socios):,}\")\n",
    "    print(f\"  ‚Ä¢ M√©dia de empresas por s√≥cio: {df_socios['qtd_empresas'].mean():.1f}\")\n",
    "    print(f\"  ‚Ä¢ M√°ximo de empresas: {df_socios['qtd_empresas'].max()}\")\n",
    "    print(f\"  ‚Ä¢ Volume total da rede: R$ {df_socios['total_recebido'].sum():,.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum s√≥cio encontrado em m√∫ltiplas empresas\")\n",
    "\n",
    "print(\"\\n‚úÖ An√°lise de rede conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2deb8-32a5-474e-87d0-ff3cadfe9f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREPARA√á√ÉO DE DADOS PARA MACHINE LEARNING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ü§ñ PREPARA√á√ÉO DE FEATURES PARA MACHINE LEARNING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Criar dataset completo\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_ml_dataset AS\n",
    "SELECT \n",
    "    cnpj,\n",
    "    -- Features num√©ricas\n",
    "    CAST(COALESCE(total_recebido_cnpj, 0) AS DOUBLE) AS feat_total_cnpj,\n",
    "    CAST(COALESCE(total_recebido_cpf, 0) AS DOUBLE) AS feat_total_cpf,\n",
    "    CAST(COALESCE(perc_recebido_cpf, 0) AS DOUBLE) AS feat_perc_cpf,\n",
    "    CAST(COALESCE(qtd_socios_recebendo, 0) AS DOUBLE) AS feat_qtd_socios,\n",
    "    CAST(COALESCE(meses_com_pagto_cnpj, 0) AS DOUBLE) AS feat_meses_cnpj,\n",
    "    CAST(COALESCE(meses_com_pagto_cpf, 0) AS DOUBLE) AS feat_meses_cpf,\n",
    "    CAST(COALESCE(pix_cnpj, 0) AS DOUBLE) AS feat_pix_cnpj,\n",
    "    CAST(COALESCE(pix_cpf, 0) AS DOUBLE) AS feat_pix_cpf,\n",
    "    CAST(COALESCE(credito_cnpj, 0) AS DOUBLE) AS feat_credito_cnpj,\n",
    "    CAST(COALESCE(credito_cpf, 0) AS DOUBLE) AS feat_credito_cpf,\n",
    "    CAST(COALESCE(debito_cnpj, 0) AS DOUBLE) AS feat_debito_cnpj,\n",
    "    CAST(COALESCE(debito_cpf, 0) AS DOUBLE) AS feat_debito_cpf,\n",
    "    \n",
    "    -- Scores\n",
    "    CAST(COALESCE(score_proporcao, 0) AS DOUBLE) AS score_proporcao,\n",
    "    CAST(COALESCE(score_volume_cpf, 0) AS DOUBLE) AS score_volume,\n",
    "    CAST(COALESCE(score_qtd_socios, 0) AS DOUBLE) AS score_socios,\n",
    "    CAST(COALESCE(score_desvio_regime, 0) AS DOUBLE) AS score_desvio,\n",
    "    CAST(COALESCE(score_consistencia, 0) AS DOUBLE) AS score_consistencia,\n",
    "    CAST(COALESCE(score_risco_final, 0) AS DOUBLE) AS score_final,\n",
    "    \n",
    "    -- Features categ√≥ricas\n",
    "    CASE \n",
    "        WHEN classificacao_risco = 'ALTO' THEN 3\n",
    "        WHEN classificacao_risco = 'M√âDIO-ALTO' THEN 2\n",
    "        WHEN classificacao_risco = 'M√âDIO' THEN 1\n",
    "        ELSE 0\n",
    "    END AS target_risco_nivel,\n",
    "    \n",
    "    CASE WHEN classificacao_risco IN ('ALTO', 'M√âDIO-ALTO') THEN 1 ELSE 0 END AS target_suspeito,\n",
    "    \n",
    "    regime_tributario,\n",
    "    uf,\n",
    "    nm_cnae1\n",
    "    \n",
    "FROM teste.dimp_score_final\n",
    "WHERE score_risco_final IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# Verificar tamanho\n",
    "total_ml = spark.sql(\"SELECT COUNT(*) as cnt FROM vw_ml_dataset\").collect()[0]['cnt']\n",
    "print(f\"\\nüìä Total de registros para ML: {total_ml:,}\")\n",
    "\n",
    "if total_ml > 0:\n",
    "    print(f\"üì• Carregando TODOS os {total_ml:,} registros...\")\n",
    "    print(\"‚ö†Ô∏è  Isso pode levar alguns minutos dependendo do volume...\")\n",
    "    \n",
    "    # Carregar dados com Spark e cachear\n",
    "    df_ml_spark = spark.sql(\"SELECT * FROM vw_ml_dataset\")\n",
    "    df_ml_spark.cache()\n",
    "    \n",
    "    # Converter para Pandas\n",
    "    print(\"üîÑ Convertendo para Pandas...\")\n",
    "    df_ml = df_ml_spark.toPandas()\n",
    "    \n",
    "    print(f\"‚úÖ Dataset carregado: {len(df_ml):,} registros √ó {len(df_ml.columns)} features\")\n",
    "    \n",
    "    # An√°lise da distribui√ß√£o do target\n",
    "    print(f\"\\nüéØ DISTRIBUI√á√ÉO DO TARGET:\")\n",
    "    dist_target = df_ml['target_suspeito'].value_counts()\n",
    "    total_records = len(df_ml)\n",
    "    print(f\"  ‚Ä¢ N√£o Suspeitos (0): {dist_target.get(0, 0):,} ({dist_target.get(0, 0)/total_records*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Suspeitos (1): {dist_target.get(1, 0):,} ({dist_target.get(1, 0)/total_records*100:.1f}%)\")\n",
    "    \n",
    "    # Estat√≠sticas descritivas (SEM usar abs() que conflita com PySpark)\n",
    "    print(f\"\\nüìä ESTAT√çSTICAS DAS FEATURES PRINCIPAIS:\\n\")\n",
    "    features_principais = ['feat_perc_cpf', 'feat_total_cpf', 'feat_qtd_socios', \n",
    "                          'score_final', 'feat_meses_cpf']\n",
    "    \n",
    "    # Criar descri√ß√£o manual para evitar conflito\n",
    "    stats_df = df_ml[features_principais].describe()\n",
    "    \n",
    "    # Formatar output manualmente\n",
    "    print(f\"{'':20s}\", end=\"\")\n",
    "    for col in features_principais:\n",
    "        print(f\"{col:>20s}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for idx in stats_df.index:\n",
    "        print(f\"{idx:20s}\", end=\"\")\n",
    "        for col in features_principais:\n",
    "            val = stats_df.loc[idx, col]\n",
    "            if np.isnan(val):\n",
    "                print(f\"{'NaN':>20s}\", end=\"\")\n",
    "            else:\n",
    "                print(f\"{val:>20,.2f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Matriz de correla√ß√£o\n",
    "    print(f\"\\nüîó CALCULANDO MATRIZ DE CORRELA√á√ÉO...\")\n",
    "    features_numericas = [col for col in df_ml.columns if col.startswith('feat_') or col.startswith('score_')]\n",
    "    \n",
    "    print(f\"   Calculando correla√ß√µes entre {len(features_numericas)} features...\")\n",
    "    corr_matrix = df_ml[features_numericas].corr()\n",
    "    \n",
    "    print(f\"‚úÖ Matriz de correla√ß√£o calculada!\")\n",
    "    \n",
    "    # Heatmap de correla√ß√£o\n",
    "    print(f\"üìä Gerando heatmap...\")\n",
    "    fig_corr = go.Figure(data=go.Heatmap(\n",
    "        z=corr_matrix.values,\n",
    "        x=corr_matrix.columns,\n",
    "        y=corr_matrix.columns,\n",
    "        colorscale='RdBu',\n",
    "        zmid=0,\n",
    "        text=np.round(corr_matrix.values, 2),\n",
    "        texttemplate='%{text}',\n",
    "        textfont={\"size\": 7},\n",
    "        hovertemplate='%{y} vs %{x}<br>Correla√ß√£o: %{z:.3f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig_corr.update_layout(\n",
    "        title='<b>Matriz de Correla√ß√£o entre Features</b>',\n",
    "        height=900,\n",
    "        width=1000,\n",
    "        xaxis=dict(tickangle=-45, tickfont=dict(size=9)),\n",
    "        yaxis=dict(tickfont=dict(size=9))\n",
    "    )\n",
    "    \n",
    "    fig_corr.show()\n",
    "    \n",
    "    # Top correla√ß√µes com o target\n",
    "    print(f\"\\nüéØ TOP 10 FEATURES MAIS CORRELACIONADAS COM TARGET:\")\n",
    "    target_corr = df_ml[features_numericas + ['target_suspeito']].corr()['target_suspeito'].drop('target_suspeito').sort_values(ascending=False)\n",
    "    \n",
    "    for idx, (feat, corr_val) in enumerate(target_corr.head(10).items(), 1):\n",
    "        print(f\"  {idx:2d}. {feat:30s} ‚Üí {corr_val:+.4f}\")\n",
    "    \n",
    "    # Distribui√ß√£o das features principais\n",
    "    print(f\"\\nüìä Gerando distribui√ß√µes das features principais...\")\n",
    "    \n",
    "    fig_dist = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=[f.replace('feat_', '').replace('_', ' ').title() for f in features_principais[:6]],\n",
    "        vertical_spacing=0.12,\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    positions = [(1,1), (1,2), (2,1), (2,2), (3,1), (3,2)]\n",
    "    colors_dist = ['#1f77b4', '#ff7f0e']\n",
    "    \n",
    "    for idx, feat in enumerate(features_principais[:6]):\n",
    "        row, col = positions[idx]\n",
    "        \n",
    "        # Histograma para cada classe\n",
    "        for class_val, color in zip([0, 1], colors_dist):\n",
    "            data_class = df_ml[df_ml['target_suspeito'] == class_val][feat]\n",
    "            \n",
    "            fig_dist.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=data_class,\n",
    "                    name=f\"Classe {class_val}\",\n",
    "                    opacity=0.6,\n",
    "                    marker=dict(color=color),\n",
    "                    showlegend=(idx == 0)\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "    \n",
    "    fig_dist.update_layout(\n",
    "        title='<b>Distribui√ß√£o das Features por Classe</b>',\n",
    "        height=800,\n",
    "        barmode='overlay',\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig_dist.show()\n",
    "    \n",
    "    # Salvar informa√ß√µes √∫teis\n",
    "    print(f\"\\nüíæ DATASET PREPARADO:\")\n",
    "    print(f\"   ‚Ä¢ Vari√°vel: df_ml\")\n",
    "    print(f\"   ‚Ä¢ Shape: {df_ml.shape}\")\n",
    "    print(f\"   ‚Ä¢ Features num√©ricas: {len(features_numericas)}\")\n",
    "    print(f\"   ‚Ä¢ Mem√≥ria utilizada: {df_ml.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Verificar valores faltantes\n",
    "    missing_summary = df_ml.isnull().sum()\n",
    "    if missing_summary.sum() > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  VALORES FALTANTES ENCONTRADOS:\")\n",
    "        for col, missing_count in missing_summary[missing_summary > 0].items():\n",
    "            print(f\"   ‚Ä¢ {col}: {missing_count:,} ({missing_count/len(df_ml)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Nenhum valor faltante encontrado!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado dispon√≠vel para ML\")\n",
    "    df_ml = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PREPARA√á√ÉO PARA ML CONCLU√çDA!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff31a2b1-d519-4ab9-8a66-f80dd9a5ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MACHINE LEARNING: CLUSTERING K-MEANS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üé≤ APRENDIZADO N√ÉO SUPERVISIONADO - K-MEANS CLUSTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if df_ml is not None and len(df_ml) > 0:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # Selecionar features para clustering\n",
    "    features_cluster = ['feat_perc_cpf', 'feat_total_cpf', 'feat_qtd_socios',\n",
    "                       'feat_meses_cpf', 'score_proporcao', 'score_volume',\n",
    "                       'score_socios', 'score_consistencia']\n",
    "    \n",
    "    df_cluster = df_ml[features_cluster].copy()\n",
    "    df_cluster = df_cluster.fillna(0)\n",
    "    \n",
    "    print(f\"\\nüìä Features selecionadas para clustering:\")\n",
    "    for feat in features_cluster:\n",
    "        print(f\"  ‚Ä¢ {feat}\")\n",
    "    \n",
    "    # Normaliza√ß√£o\n",
    "    print(f\"\\n‚öôÔ∏è  Normalizando features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df_cluster)\n",
    "    \n",
    "    # M√©todo do Cotovelo para determinar K\n",
    "    print(f\"\\nüìà Calculando m√©todo do cotovelo...\")\n",
    "    inertias = []\n",
    "    K_range = range(2, 11)\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans_temp.fit(X_scaled)\n",
    "        inertias.append(kmeans_temp.inertia_)\n",
    "    \n",
    "    # Gr√°fico do cotovelo\n",
    "    fig_elbow = go.Figure(data=go.Scatter(\n",
    "        x=list(K_range),\n",
    "        y=inertias,\n",
    "        mode='lines+markers',\n",
    "        marker=dict(size=10, color='#1f77b4'),\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "    \n",
    "    fig_elbow.update_layout(\n",
    "        title='<b>M√©todo do Cotovelo - Determina√ß√£o do K Ideal</b>',\n",
    "        xaxis_title='N√∫mero de Clusters (K)',\n",
    "        yaxis_title='In√©rcia (Soma dos Quadrados Intra-Cluster)',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig_elbow.show()\n",
    "    \n",
    "    # Aplicar K-Means com k=4\n",
    "    k_optimal = 4\n",
    "    print(f\"\\nüéØ Aplicando K-Means com K={k_optimal}...\")\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "    df_ml['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # An√°lise dos clusters\n",
    "    print(f\"\\nüìä AN√ÅLISE DOS CLUSTERS:\\n\")\n",
    "    for cluster_id in range(k_optimal):\n",
    "        cluster_data = df_ml[df_ml['cluster'] == cluster_id]\n",
    "        print(f\"Cluster {cluster_id}:\")\n",
    "        print(f\"  ‚Ä¢ Tamanho: {len(cluster_data):,} empresas ({len(cluster_data)/len(df_ml)*100:.1f}%)\")\n",
    "        print(f\"  ‚Ä¢ % CPF M√©dio: {cluster_data['feat_perc_cpf'].mean():.1f}%\")\n",
    "        print(f\"  ‚Ä¢ Volume CPF M√©dio: R$ {cluster_data['feat_total_cpf'].mean():,.2f}\")\n",
    "        print(f\"  ‚Ä¢ Score M√©dio: {cluster_data['score_final'].mean():.2f}\")\n",
    "        print(f\"  ‚Ä¢ % Suspeitos: {(cluster_data['target_suspeito'].sum()/len(cluster_data)*100):.1f}%\")\n",
    "        print()\n",
    "    \n",
    "    # PCA para visualiza√ß√£o 2D\n",
    "    print(f\"üîÑ Reduzindo dimensionalidade com PCA...\")\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    df_ml['pca1'] = X_pca[:, 0]\n",
    "    df_ml['pca2'] = X_pca[:, 1]\n",
    "    \n",
    "    print(f\"‚úÖ Vari√¢ncia explicada: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "    \n",
    "    # Visualiza√ß√£o dos clusters\n",
    "    fig_clusters = go.Figure()\n",
    "    \n",
    "    colors_cluster = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    \n",
    "    for cluster_id in range(k_optimal):\n",
    "        cluster_data = df_ml[df_ml['cluster'] == cluster_id]\n",
    "        \n",
    "        fig_clusters.add_trace(go.Scatter(\n",
    "            x=cluster_data['pca1'],\n",
    "            y=cluster_data['pca2'],\n",
    "            mode='markers',\n",
    "            name=f'Cluster {cluster_id}',\n",
    "            marker=dict(\n",
    "                size=6,\n",
    "                color=colors_cluster[cluster_id],\n",
    "                opacity=0.6\n",
    "            ),\n",
    "            text=cluster_data['cnpj'],\n",
    "            hovertemplate='<b>Cluster %{fullData.name}</b><br>CNPJ: %{text}<br>PCA1: %{x:.2f}<br>PCA2: %{y:.2f}<extra></extra>'\n",
    "        ))\n",
    "    \n",
    "    # Centroides\n",
    "    centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
    "    fig_clusters.add_trace(go.Scatter(\n",
    "        x=centroids_pca[:, 0],\n",
    "        y=centroids_pca[:, 1],\n",
    "        mode='markers',\n",
    "        name='Centroides',\n",
    "        marker=dict(\n",
    "            size=15,\n",
    "            color='black',\n",
    "            symbol='x',\n",
    "            line=dict(width=2, color='white')\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    fig_clusters.update_layout(\n",
    "        title='<b>Visualiza√ß√£o dos Clusters (PCA 2D)</b>',\n",
    "        xaxis_title=f'Componente Principal 1 ({pca.explained_variance_ratio_[0]*100:.1f}%)',\n",
    "        yaxis_title=f'Componente Principal 2 ({pca.explained_variance_ratio_[1]*100:.1f}%)',\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig_clusters.show()\n",
    "    \n",
    "    # Perfil dos clusters\n",
    "    fig_perfil = go.Figure()\n",
    "    \n",
    "    features_perfil = ['feat_perc_cpf', 'feat_total_cpf', 'feat_qtd_socios', 'score_final']\n",
    "    df_perfil = df_ml.groupby('cluster')[features_perfil].mean()\n",
    "    \n",
    "    # Normalizar para visualiza√ß√£o\n",
    "    df_perfil_norm = (df_perfil - df_perfil.min()) / (df_perfil.max() - df_perfil.min())\n",
    "    \n",
    "    for cluster_id in range(k_optimal):\n",
    "        fig_perfil.add_trace(go.Scatterpolar(\n",
    "            r=df_perfil_norm.loc[cluster_id].values,\n",
    "            theta=features_perfil,\n",
    "            fill='toself',\n",
    "            name=f'Cluster {cluster_id}',\n",
    "            line=dict(color=colors_cluster[cluster_id])\n",
    "        ))\n",
    "    \n",
    "    fig_perfil.update_layout(\n",
    "        polar=dict(radialaxis=dict(visible=True, range=[0, 1])),\n",
    "        title='<b>Perfil dos Clusters (Normalizado)</b>',\n",
    "        height=500,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig_perfil.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dataset n√£o dispon√≠vel para clustering\")\n",
    "\n",
    "print(\"\\n‚úÖ Clustering K-Means conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a0b4b-2251-4a6e-93df-d548104a36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MACHINE LEARNING: RANDOM FOREST CLASSIFIER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üå≤ APRENDIZADO SUPERVISIONADO - RANDOM FOREST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if df_ml is not None and len(df_ml) > 0:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "    \n",
    "    # Preparar features e target\n",
    "    features_rf = ['feat_perc_cpf', 'feat_total_cpf', 'feat_qtd_socios',\n",
    "                   'feat_meses_cpf', 'feat_pix_cpf', 'feat_credito_cpf',\n",
    "                   'score_proporcao', 'score_volume', 'score_socios',\n",
    "                   'score_desvio', 'score_consistencia']\n",
    "    \n",
    "    X = df_ml[features_rf].fillna(0)\n",
    "    y = df_ml['target_suspeito']\n",
    "    \n",
    "    print(f\"\\nüìä Dataset para treinamento:\")\n",
    "    print(f\"  ‚Ä¢ Features: {len(features_rf)}\")\n",
    "    print(f\"  ‚Ä¢ Amostras: {len(X):,}\")\n",
    "    print(f\"  ‚Ä¢ Distribui√ß√£o: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Split treino/teste\n",
    "    print(f\"\\n‚úÇÔ∏è  Dividindo em treino (70%) e teste (30%)...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Treino: {len(X_train):,} amostras\")\n",
    "    print(f\"  ‚Ä¢ Teste: {len(X_test):,} amostras\")\n",
    "    \n",
    "    # Treinar modelo\n",
    "    print(f\"\\nüå≤ Treinando Random Forest...\")\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    print(f\"‚úÖ Modelo treinado!\")\n",
    "    \n",
    "    # Predi√ß√µes\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # M√©tricas\n",
    "    print(f\"\\nüìä M√âTRICAS DE DESEMPENHO:\\n\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['N√£o Suspeito', 'Suspeito']))\n",
    "    \n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"\\nüéØ AUC-ROC Score: {auc_score:.4f}\")\n",
    "    \n",
    "    # Matriz de confus√£o\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    fig_cm = go.Figure(data=go.Heatmap(\n",
    "        z=cm,\n",
    "        x=['N√£o Suspeito', 'Suspeito'],\n",
    "        y=['N√£o Suspeito', 'Suspeito'],\n",
    "        text=cm,\n",
    "        texttemplate='%{text}',\n",
    "        textfont={\"size\": 16},\n",
    "        colorscale='Blues'\n",
    "    ))\n",
    "    \n",
    "    fig_cm.update_layout(\n",
    "        title='<b>Matriz de Confus√£o - Random Forest</b>',\n",
    "        xaxis_title='Predi√ß√£o',\n",
    "        yaxis_title='Real',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig_cm.show()\n",
    "    \n",
    "    # Curva ROC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    fig_roc = go.Figure()\n",
    "    \n",
    "    fig_roc.add_trace(go.Scatter(\n",
    "        x=fpr, y=tpr,\n",
    "        mode='lines',\n",
    "        name=f'Random Forest (AUC = {auc_score:.3f})',\n",
    "        line=dict(color='#1f77b4', width=2)\n",
    "    ))\n",
    "    \n",
    "    fig_roc.add_trace(go.Scatter(\n",
    "        x=[0, 1], y=[0, 1],\n",
    "        mode='lines',\n",
    "        name='Baseline (Random)',\n",
    "        line=dict(color='red', width=1, dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig_roc.update_layout(\n",
    "        title='<b>Curva ROC - Random Forest</b>',\n",
    "        xaxis_title='Taxa de Falsos Positivos',\n",
    "        yaxis_title='Taxa de Verdadeiros Positivos',\n",
    "        height=500,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig_roc.show()\n",
    "    \n",
    "    # Import√¢ncia das features\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features_rf,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä IMPORT√ÇNCIA DAS FEATURES:\\n\")\n",
    "    # ‚úÖ CORRE√á√ÉO: Imprimir manualmente para evitar conflito com pd.set_option\n",
    "    for idx, row in feature_importance.iterrows():\n",
    "        print(f\"  {row['feature']:30s} ‚Üí {row['importance']:.6f}\")\n",
    "    \n",
    "    fig_importance = go.Figure(go.Bar(\n",
    "        y=feature_importance['feature'],\n",
    "        x=feature_importance['importance'],\n",
    "        orientation='h',\n",
    "        marker=dict(color=feature_importance['importance'], colorscale='Viridis'),\n",
    "        text=feature_importance['importance'].apply(lambda x: f'{x:.4f}'),\n",
    "        textposition='outside'\n",
    "    ))\n",
    "    \n",
    "    fig_importance.update_layout(\n",
    "        title='<b>Import√¢ncia das Features - Random Forest</b>',\n",
    "        xaxis_title='Import√¢ncia',\n",
    "        yaxis_title='Feature',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig_importance.show()\n",
    "    \n",
    "    # Adicionar probabilidades ao dataset original\n",
    "    df_ml['rf_probability'] = np.nan\n",
    "    df_ml.loc[X_test.index, 'rf_probability'] = y_pred_proba\n",
    "    \n",
    "    print(f\"\\nüíæ Probabilidades adicionadas ao dataset 'df_ml'\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dataset n√£o dispon√≠vel para Random Forest\")\n",
    "\n",
    "print(\"\\n‚úÖ Random Forest conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2ea15-4dbc-4247-8faa-f4db3b4556e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MACHINE LEARNING: XGBOOST CLASSIFIER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚ö° APRENDIZADO SUPERVISIONADO - XGBOOST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if df_ml is not None and len(df_ml) > 0:\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "    \n",
    "    # Usar as mesmas features do Random Forest\n",
    "    X = df_ml[features_rf].fillna(0)\n",
    "    y = df_ml['target_suspeito']\n",
    "    \n",
    "    # Usar o mesmo split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è  Configurando XGBoost...\")\n",
    "    \n",
    "    # Calcular scale_pos_weight para desbalanceamento\n",
    "    scale_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    print(f\"  ‚Ä¢ Scale Pos Weight: {scale_weight:.2f}\")\n",
    "    \n",
    "    # Treinar modelo\n",
    "    print(f\"\\n‚ö° Treinando XGBoost...\")\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=scale_weight,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    print(f\"‚úÖ Modelo treinado!\")\n",
    "    \n",
    "    # Predi√ß√µes\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # M√©tricas\n",
    "    print(f\"\\nüìä M√âTRICAS DE DESEMPENHO - XGBOOST:\\n\")\n",
    "    print(classification_report(y_test, y_pred_xgb, target_names=['N√£o Suspeito', 'Suspeito']))\n",
    "    \n",
    "    auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "    print(f\"\\nüéØ AUC-ROC Score: {auc_xgb:.4f}\")\n",
    "    \n",
    "    # Compara√ß√£o RF vs XGBoost\n",
    "    print(f\"\\nüìä COMPARA√á√ÉO DE MODELOS:\")\n",
    "    print(f\"  ‚Ä¢ Random Forest AUC: {auc_score:.4f}\")\n",
    "    print(f\"  ‚Ä¢ XGBoost AUC: {auc_xgb:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Diferen√ßa: {np.abs(auc_xgb - auc_score):.4f}\")\n",
    "    \n",
    "    if auc_xgb > auc_score:\n",
    "        print(f\"  ‚úÖ XGBoost √© {((auc_xgb/auc_score - 1)*100):.2f}% melhor\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Random Forest √© {((auc_score/auc_xgb - 1)*100):.2f}% melhor\")\n",
    "    \n",
    "    # Curvas ROC comparadas\n",
    "    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "    \n",
    "    fig_compare = go.Figure()\n",
    "    \n",
    "    fig_compare.add_trace(go.Scatter(\n",
    "        x=fpr, y=tpr,\n",
    "        mode='lines',\n",
    "        name=f'Random Forest (AUC = {auc_score:.3f})',\n",
    "        line=dict(color='#1f77b4', width=2)\n",
    "    ))\n",
    "    \n",
    "    fig_compare.add_trace(go.Scatter(\n",
    "        x=fpr_xgb, y=tpr_xgb,\n",
    "        mode='lines',\n",
    "        name=f'XGBoost (AUC = {auc_xgb:.3f})',\n",
    "        line=dict(color='#ff7f0e', width=2)\n",
    "    ))\n",
    "    \n",
    "    fig_compare.add_trace(go.Scatter(\n",
    "        x=[0, 1], y=[0, 1],\n",
    "        mode='lines',\n",
    "        name='Baseline (Random)',\n",
    "        line=dict(color='red', width=1, dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig_compare.update_layout(\n",
    "        title='<b>Compara√ß√£o: Random Forest vs XGBoost</b>',\n",
    "        xaxis_title='Taxa de Falsos Positivos',\n",
    "        yaxis_title='Taxa de Verdadeiros Positivos',\n",
    "        height=500,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig_compare.show()\n",
    "    \n",
    "    # Matriz de confus√£o XGBoost\n",
    "    cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "    \n",
    "    fig_cm_xgb = go.Figure(data=go.Heatmap(\n",
    "        z=cm_xgb,\n",
    "        x=['N√£o Suspeito', 'Suspeito'],\n",
    "        y=['N√£o Suspeito', 'Suspeito'],\n",
    "        text=cm_xgb,\n",
    "        texttemplate='%{text}',\n",
    "        textfont={\"size\": 16},\n",
    "        colorscale='Oranges'\n",
    "    ))\n",
    "    \n",
    "    fig_cm_xgb.update_layout(\n",
    "        title='<b>Matriz de Confus√£o - XGBoost</b>',\n",
    "        xaxis_title='Predi√ß√£o',\n",
    "        yaxis_title='Real',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig_cm_xgb.show()\n",
    "    \n",
    "    # Import√¢ncia das features - XGBoost\n",
    "    feature_importance_xgb = pd.DataFrame({\n",
    "        'feature': features_rf,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä IMPORT√ÇNCIA DAS FEATURES - XGBOOST:\\n\")\n",
    "    for idx, row in feature_importance_xgb.iterrows():\n",
    "        print(f\"  {row['feature']:30s} ‚Üí {row['importance']:.6f}\")\n",
    "    \n",
    "    fig_importance_xgb = go.Figure(go.Bar(\n",
    "        y=feature_importance_xgb['feature'],\n",
    "        x=feature_importance_xgb['importance'],\n",
    "        orientation='h',\n",
    "        marker=dict(color=feature_importance_xgb['importance'], colorscale='Plasma'),\n",
    "        text=feature_importance_xgb['importance'].apply(lambda x: f'{x:.4f}'),\n",
    "        textposition='outside'\n",
    "    ))\n",
    "    \n",
    "    fig_importance_xgb.update_layout(\n",
    "        title='<b>Import√¢ncia das Features - XGBoost</b>',\n",
    "        xaxis_title='Import√¢ncia',\n",
    "        yaxis_title='Feature',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig_importance_xgb.show()\n",
    "    \n",
    "    # Compara√ß√£o de import√¢ncia\n",
    "    fig_comp_importance = go.Figure()\n",
    "    \n",
    "    fig_comp_importance.add_trace(go.Bar(\n",
    "        name='Random Forest',\n",
    "        y=feature_importance['feature'],\n",
    "        x=feature_importance['importance'],\n",
    "        orientation='h',\n",
    "        marker=dict(color='#1f77b4')\n",
    "    ))\n",
    "    \n",
    "    fig_comp_importance.add_trace(go.Bar(\n",
    "        name='XGBoost',\n",
    "        y=feature_importance_xgb['feature'],\n",
    "        x=feature_importance_xgb['importance'],\n",
    "        orientation='h',\n",
    "        marker=dict(color='#ff7f0e')\n",
    "    ))\n",
    "    \n",
    "    fig_comp_importance.update_layout(\n",
    "        title='<b>Compara√ß√£o: Import√¢ncia das Features (RF vs XGBoost)</b>',\n",
    "        xaxis_title='Import√¢ncia',\n",
    "        yaxis_title='Feature',\n",
    "        height=600,\n",
    "        barmode='group'\n",
    "    )\n",
    "    \n",
    "    fig_comp_importance.show()\n",
    "    \n",
    "# Adicionar probabilidades XGBoost ao dataset\n",
    "    df_ml['xgb_probability'] = np.nan\n",
    "    df_ml.loc[X_test.index, 'xgb_probability'] = y_pred_proba_xgb\n",
    "    \n",
    "    # Ensemble (m√©dia das probabilidades)\n",
    "    df_ml['ensemble_probability'] = (df_ml['rf_probability'] + df_ml['xgb_probability']) / 2\n",
    "    \n",
    "    print(f\"\\nüíæ Probabilidades XGBoost e Ensemble adicionadas ao dataset 'df_ml'\")\n",
    "    \n",
    "    # An√°lise do Ensemble\n",
    "    y_pred_ensemble = (df_ml.loc[X_test.index, 'ensemble_probability'] >= 0.5).astype(int)\n",
    "    auc_ensemble = roc_auc_score(y_test, df_ml.loc[X_test.index, 'ensemble_probability'])\n",
    "    \n",
    "    print(f\"\\nüéØ COMPARA√á√ÉO FINAL DOS MODELOS:\")\n",
    "    print(f\"  ‚Ä¢ Random Forest AUC: {auc_score:.4f}\")\n",
    "    print(f\"  ‚Ä¢ XGBoost AUC: {auc_xgb:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Ensemble (M√©dia) AUC: {auc_ensemble:.4f}\")\n",
    "    \n",
    "    # ‚úÖ CORRE√á√ÉO: Usar max do Python explicitamente\n",
    "    import builtins\n",
    "    modelos_comparacao = [\n",
    "        ('Random Forest', auc_score), \n",
    "        ('XGBoost', auc_xgb), \n",
    "        ('Ensemble', auc_ensemble)\n",
    "    ]\n",
    "    best_model = builtins.max(modelos_comparacao, key=lambda x: x[1])\n",
    "    print(f\"\\nüèÜ MELHOR MODELO: {best_model[0]} (AUC = {best_model[1]:.4f})\")\n",
    "    \n",
    "    # ‚ö†Ô∏è ALERTA: AUC = 1.0 indica poss√≠vel overfitting!\n",
    "    if auc_score >= 0.99 or auc_xgb >= 0.99:\n",
    "        print(f\"\\n‚ö†Ô∏è  ATEN√á√ÉO: AUC perfeito ou pr√≥ximo de 1.0 detectado!\")\n",
    "        print(f\"    Isso pode indicar:\")\n",
    "        print(f\"    ‚Ä¢ Overfitting nos dados de treino\")\n",
    "        print(f\"    ‚Ä¢ Vazamento de dados (data leakage)\")\n",
    "        print(f\"    ‚Ä¢ Features que cont√™m informa√ß√£o do target\")\n",
    "        print(f\"\\n    üí° RECOMENDA√á√ïES:\")\n",
    "        print(f\"    1. Revisar as features usadas\")\n",
    "        print(f\"    2. Verificar se score_final ou componentes derivam do target\")\n",
    "        print(f\"    3. Usar apenas features independentes (feat_*)\")\n",
    "        print(f\"    4. Aplicar valida√ß√£o cruzada\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dataset n√£o dispon√≠vel para XGBoost\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ XGBoost conclu√≠do!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be919a89-12ff-4c70-937b-faccfd6db887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RETREINAMENTO - CORRIGINDO DATA LEAKAGE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîß RETREINAMENTO DOS MODELOS - SEM DATA LEAKAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if df_ml is not None and len(df_ml) > 0:\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  PROBLEMA IDENTIFICADO:\")\n",
    "    print(\"   Os scores (score_proporcao, score_volume, etc.) foram calculados\")\n",
    "    print(\"   a partir do target (classificacao_risco), causando data leakage!\")\n",
    "    print(\"\\n‚úÖ SOLU√á√ÉO:\")\n",
    "    print(\"   Usar apenas features independentes (feat_*)\")\n",
    "    \n",
    "    # ‚úÖ Features INDEPENDENTES (sem data leakage)\n",
    "    features_clean = [\n",
    "        'feat_perc_cpf',\n",
    "        'feat_total_cpf',\n",
    "        'feat_total_cnpj',\n",
    "        'feat_qtd_socios',\n",
    "        'feat_meses_cpf',\n",
    "        'feat_meses_cnpj',\n",
    "        'feat_pix_cpf',\n",
    "        'feat_pix_cnpj',\n",
    "        'feat_credito_cpf',\n",
    "        'feat_credito_cnpj',\n",
    "        'feat_debito_cpf',\n",
    "        'feat_debito_cnpj'\n",
    "    ]\n",
    "    \n",
    "    X_clean = df_ml[features_clean].fillna(0)\n",
    "    y_clean = df_ml['target_suspeito']\n",
    "    \n",
    "    print(f\"\\nüìä Dataset limpo:\")\n",
    "    print(f\"  ‚Ä¢ Features independentes: {len(features_clean)}\")\n",
    "    print(f\"  ‚Ä¢ Amostras: {len(X_clean):,}\")\n",
    "    \n",
    "    # Split\n",
    "    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "        X_clean, y_clean, test_size=0.3, random_state=42, stratify=y_clean\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # RANDOM FOREST - Retreinamento\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\nüå≤ Treinando Random Forest (sem leakage)...\")\n",
    "    rf_clean = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_clean.fit(X_train_c, y_train_c)\n",
    "    y_pred_rf_c = rf_clean.predict(X_test_c)\n",
    "    y_proba_rf_c = rf_clean.predict_proba(X_test_c)[:, 1]\n",
    "    auc_rf_clean = roc_auc_score(y_test_c, y_proba_rf_c)\n",
    "    \n",
    "    print(f\"‚úÖ Random Forest treinado! AUC = {auc_rf_clean:.4f}\")\n",
    "    \n",
    "    # Valida√ß√£o cruzada\n",
    "    print(f\"\\nüîÑ Valida√ß√£o cruzada (5-fold)...\")\n",
    "    cv_scores_rf = cross_val_score(rf_clean, X_clean, y_clean, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    print(f\"   AUC m√©dio CV: {cv_scores_rf.mean():.4f} (¬±{cv_scores_rf.std():.4f})\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # XGBOOST - Retreinamento\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n‚ö° Treinando XGBoost (sem leakage)...\")\n",
    "    scale_weight_c = (y_train_c == 0).sum() / (y_train_c == 1).sum()\n",
    "    \n",
    "    xgb_clean = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=scale_weight_c,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    xgb_clean.fit(X_train_c, y_train_c)\n",
    "    y_pred_xgb_c = xgb_clean.predict(X_test_c)\n",
    "    y_proba_xgb_c = xgb_clean.predict_proba(X_test_c)[:, 1]\n",
    "    auc_xgb_clean = roc_auc_score(y_test_c, y_proba_xgb_c)\n",
    "    \n",
    "    print(f\"‚úÖ XGBoost treinado! AUC = {auc_xgb_clean:.4f}\")\n",
    "    \n",
    "    # Valida√ß√£o cruzada\n",
    "    print(f\"\\nüîÑ Valida√ß√£o cruzada (5-fold)...\")\n",
    "    cv_scores_xgb = cross_val_score(xgb_clean, X_clean, y_clean, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    print(f\"   AUC m√©dio CV: {cv_scores_xgb.mean():.4f} (¬±{cv_scores_xgb.std():.4f})\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # COMPARA√á√ÉO: COM vs SEM Data Leakage\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä COMPARA√á√ÉO: MODELOS COM DATA LEAKAGE vs MODELOS LIMPOS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n{'Modelo':<25s} {'AUC (com leakage)':<20s} {'AUC (limpo)':<15s}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Random Forest':<25s} {auc_score:>15.4f}     {auc_rf_clean:>15.4f}\")\n",
    "    print(f\"{'XGBoost':<25s} {auc_xgb:>15.4f}     {auc_xgb_clean:>15.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # M√âTRICAS DETALHADAS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\nüìä M√âTRICAS DETALHADAS - RANDOM FOREST (LIMPO):\\n\")\n",
    "    print(classification_report(y_test_c, y_pred_rf_c, target_names=['N√£o Suspeito', 'Suspeito']))\n",
    "    \n",
    "    print(f\"\\nüìä M√âTRICAS DETALHADAS - XGBOOST (LIMPO):\\n\")\n",
    "    print(classification_report(y_test_c, y_pred_xgb_c, target_names=['N√£o Suspeito', 'Suspeito']))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # GR√ÅFICOS COMPARATIVOS\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Curvas ROC\n",
    "    fpr_rf_c, tpr_rf_c, _ = roc_curve(y_test_c, y_proba_rf_c)\n",
    "    fpr_xgb_c, tpr_xgb_c, _ = roc_curve(y_test_c, y_proba_xgb_c)\n",
    "    \n",
    "    fig_roc_clean = go.Figure()\n",
    "    \n",
    "    fig_roc_clean.add_trace(go.Scatter(\n",
    "        x=fpr_rf_c, y=tpr_rf_c,\n",
    "        mode='lines',\n",
    "        name=f'Random Forest (AUC={auc_rf_clean:.3f})',\n",
    "        line=dict(color='#1f77b4', width=2)\n",
    "    ))\n",
    "    \n",
    "    fig_roc_clean.add_trace(go.Scatter(\n",
    "        x=fpr_xgb_c, y=tpr_xgb_c,\n",
    "        mode='lines',\n",
    "        name=f'XGBoost (AUC={auc_xgb_clean:.3f})',\n",
    "        line=dict(color='#ff7f0e', width=2)\n",
    "    ))\n",
    "    \n",
    "    fig_roc_clean.add_trace(go.Scatter(\n",
    "        x=[0, 1], y=[0, 1],\n",
    "        mode='lines',\n",
    "        name='Baseline',\n",
    "        line=dict(color='red', width=1, dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig_roc_clean.update_layout(\n",
    "        title='<b>Curvas ROC - Modelos Limpos (Sem Data Leakage)</b>',\n",
    "        xaxis_title='Taxa de Falsos Positivos',\n",
    "        yaxis_title='Taxa de Verdadeiros Positivos',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig_roc_clean.show()\n",
    "    \n",
    "    # Import√¢ncia das features\n",
    "    importance_rf_c = pd.DataFrame({\n",
    "        'feature': features_clean,\n",
    "        'importance': rf_clean.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    importance_xgb_c = pd.DataFrame({\n",
    "        'feature': features_clean,\n",
    "        'importance': xgb_clean.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä TOP 10 FEATURES MAIS IMPORTANTES:\\n\")\n",
    "    print(\"Random Forest:\")\n",
    "    for idx, row in importance_rf_c.head(10).iterrows():\n",
    "        print(f\"  {row['feature']:30s} ‚Üí {row['importance']:.6f}\")\n",
    "    \n",
    "    print(\"\\nXGBoost:\")\n",
    "    for idx, row in importance_xgb_c.head(10).iterrows():\n",
    "        print(f\"  {row['feature']:30s} ‚Üí {row['importance']:.6f}\")\n",
    "    \n",
    "    # Gr√°fico comparativo de import√¢ncia\n",
    "    fig_importance_comp = go.Figure()\n",
    "    \n",
    "    fig_importance_comp.add_trace(go.Bar(\n",
    "        name='Random Forest',\n",
    "        y=importance_rf_c['feature'],\n",
    "        x=importance_rf_c['importance'],\n",
    "        orientation='h',\n",
    "        marker=dict(color='#1f77b4')\n",
    "    ))\n",
    "    \n",
    "    fig_importance_comp.add_trace(go.Bar(\n",
    "        name='XGBoost',\n",
    "        y=importance_xgb_c['feature'],\n",
    "        x=importance_xgb_c['importance'],\n",
    "        orientation='h',\n",
    "        marker=dict(color='#ff7f0e')\n",
    "    ))\n",
    "    \n",
    "    fig_importance_comp.update_layout(\n",
    "        title='<b>Import√¢ncia das Features - Modelos Limpos</b>',\n",
    "        xaxis_title='Import√¢ncia',\n",
    "        yaxis_title='Feature',\n",
    "        height=600,\n",
    "        barmode='group'\n",
    "    )\n",
    "    \n",
    "    fig_importance_comp.show()\n",
    "    \n",
    "    # Salvar modelos limpos\n",
    "    df_ml['rf_clean_probability'] = np.nan\n",
    "    df_ml.loc[X_test_c.index, 'rf_clean_probability'] = y_proba_rf_c\n",
    "    \n",
    "    df_ml['xgb_clean_probability'] = np.nan\n",
    "    df_ml.loc[X_test_c.index, 'xgb_clean_probability'] = y_proba_xgb_c\n",
    "    \n",
    "    print(f\"\\nüíæ Probabilidades dos modelos limpos adicionadas ao dataset!\")\n",
    "    \n",
    "    # Melhor modelo limpo\n",
    "    import builtins\n",
    "    modelos_limpos = [\n",
    "        ('Random Forest', auc_rf_clean, cv_scores_rf.mean()),\n",
    "        ('XGBoost', auc_xgb_clean, cv_scores_xgb.mean())\n",
    "    ]\n",
    "    best_clean = builtins.max(modelos_limpos, key=lambda x: x[2])  # Por CV score\n",
    "    \n",
    "    print(f\"\\nüèÜ MELHOR MODELO (baseado em CV): {best_clean[0]}\")\n",
    "    print(f\"   AUC Test: {best_clean[1]:.4f}\")\n",
    "    print(f\"   AUC CV: {best_clean[2]:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Dataset n√£o dispon√≠vel\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ RETREINAMENTO CONCLU√çDO!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8589e8c5-6aa9-4e3d-95d1-35f675bfa596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Pipeline)",
   "language": "python",
   "name": "conda_data_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
